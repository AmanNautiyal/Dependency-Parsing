{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Graph_Based_Dependency_Parsing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OClWFsaISHL7"
      },
      "source": [
        "filepath='/content/sample_data/hi_hdtb-ud-test.conllu'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Le1iHwJPSf4q"
      },
      "source": [
        "**Data Cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjrjaXWKSRyl"
      },
      "source": [
        "def clean_conllu_file():\n",
        "    with open(filepath, 'r') as f:\n",
        "        data = f.readlines()\n",
        "    output = ''\n",
        "    for line in data:\n",
        "        if line == '\\n':\n",
        "            continue\n",
        "        if line.startswith('# sent_id'):\n",
        "            output += '\\n'\n",
        "        output += line\n",
        "    print(output[:100])\n",
        "    with open('/content/sample_data/DATA/newfile.conllu', 'w+') as f:\n",
        "        f.write(output)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QWryOEmSfTt",
        "outputId": "a27475f3-6845-49c1-833f-3958fbdaa6c7"
      },
      "source": [
        "clean_conllu_file()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# sent_id = test-s1\n",
            "# text = इसके अतिरिक्त गुग्गुल कुंड, भीम गुफा तथा भीमशिला भी दर्शनीय स्थल हैं ।\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxUHe54rSyih"
      },
      "source": [
        "**Converting a conllu file to json**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsAh98IoSs9u"
      },
      "source": [
        "import pprint\n",
        "import os\n",
        "import json\n",
        "import copy\n",
        "def conllu_to_json(filepath=None):\n",
        "    # initialize\n",
        "    text = []\n",
        "    # Read conllu file\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        source = f.read()\n",
        "    sentences = source.strip().split('\\n\\n')\n",
        "    for sentence in sentences:\n",
        "        temp_lines = sentence.strip().split('\\n')\n",
        "        sent = {}\n",
        "        lines = []\n",
        "        for line in temp_lines:\n",
        "            words = line.split(' ')\n",
        "            # Drop all lines beginning with #\n",
        "            if words[0] == '#':\n",
        "                if words[1] == 'sent_id':\n",
        "                    sent['sent_id'] = words[3]\n",
        "            else:\n",
        "                lines.append(line)\n",
        "        reject_sentence = False\n",
        "        words = []\n",
        "        for line in lines:\n",
        "            words_list = line.split('\\t')\n",
        "            try:\n",
        "                int(words_list[0])\n",
        "            except ValueError:\n",
        "                reject_sentence = True\n",
        "                break\n",
        "\n",
        "            word = {\n",
        "                \"id\": words_list[0],\n",
        "                \"form\": words_list[1],\n",
        "                \"lemma\": words_list[2],\n",
        "                \"upostag\": words_list[3],\n",
        "                \"xpostag\": words_list[4],\n",
        "                \"feats\": words_list[5],\n",
        "                \"head\": words_list[6],\n",
        "                \"deprel\": words_list[7],\n",
        "                \"deps\": words_list[8],\n",
        "                \"misc\": words_list[9]\n",
        "            }\n",
        "            words.append(word)\n",
        "\n",
        "            if word['deprel'] == 'root':\n",
        "                words.append({\n",
        "                    \"id\": \"0\",\n",
        "                    \"form\": \"<ROOT> \",\n",
        "                    \"lemma\": \"<ROOT>\",\n",
        "                    \"upostag\": \"ROOT\",\n",
        "                    \"xpostag\": \"ROOT\",\n",
        "                    \"feats\": \"_\",\n",
        "                    \"head\": \"-1\",\n",
        "                    \"deprel\": \"_\",\n",
        "                    \"deps\": \"_\",\n",
        "                    \"misc\": \"_\"\n",
        "                })\n",
        "\n",
        "        if reject_sentence:\n",
        "            continue\n",
        "\n",
        "        sent['words'] = words\n",
        "        text.append(sent)\n",
        "\n",
        "    with open(filepath.replace('conllu', 'json'), 'w+') as f:\n",
        "        f.write(json.dumps(text, indent=4))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wL7eQ4QyTEKb"
      },
      "source": [
        "conllu_to_json('/content/sample_data/DATA/newfile.conllu')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ad8kIeodTHLR"
      },
      "source": [
        "import json\n",
        "import os\n",
        "import copy\n",
        "import torch\n",
        "import pickle\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.autograd as autograd\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from time import gmtime, strftime\n",
        "from torch.autograd import Variable\n",
        "from gensim.models import Word2Vec"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9QqQmkBTkJI"
      },
      "source": [
        "**Word Embedding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ewUKvlnTQ_v"
      },
      "source": [
        "def pretrain_word_embeddings(data, len_word_embed, len_pos_embed):\n",
        "    '''\n",
        "    corpus_words in the following shape:\n",
        "    [ [some sentence]\n",
        "      [some other sentence] ]\n",
        "    all_words in the shape:\n",
        "    [ some sentence some other sentence]\n",
        "    corpus_words in Format: [['I', 'like', 'custard'],...]\n",
        "    corpus_pos in Format: [['NN', 'VB', 'PRN'],...]\n",
        "    '''\n",
        "    corpus_words = []\n",
        "    corpus_pos = []\n",
        "    all_words = []\n",
        "    all_pos = []\n",
        "    l2i = {}\n",
        "    for sentence in data:\n",
        "        words = []\n",
        "        pos_s = []\n",
        "        for word in sentence['words']:\n",
        "            words.append(word['form'])\n",
        "            pos_s.append(word['xpostag'])\n",
        "            label = word['deprel']\n",
        "            if label not in l2i:\n",
        "                l2i[label] = len(l2i)\n",
        "        corpus_words.append(words)\n",
        "        corpus_pos.append(pos_s)\n",
        "        all_words.extend(words)\n",
        "        all_pos.extend(pos_s)\n",
        "    with open('/content/sample_data/DATA/labels.json', 'w+') as f:\n",
        "        f.write(json.dumps(l2i, indent=4))\n",
        "    w2i = {word: idx for idx, word in enumerate(all_words)}\n",
        "    p2i = {pos: idx for idx, pos in enumerate(all_pos)}\n",
        "\n",
        "    # pre-train word and pos embeddings. These will be starting points for our learnable embeddings\n",
        "    word_embeddings_gensim = Word2Vec(corpus_words, size=len_word_embed, window=5, min_count=1, workers=8)\n",
        "    pos_embeddings_gensim = Word2Vec(corpus_pos, size=len_pos_embed, window=5, min_count=1, workers=8)\n",
        "\n",
        "    # initialise the embeddings. The tensors are still empty\n",
        "    pretrained_word_embeddings = torch.FloatTensor(max(w2i.values())+1, len_word_embed)\n",
        "    pretrained_pos_embeddings = torch.FloatTensor(max(p2i.values())+1, len_pos_embed)\n",
        "\n",
        "    # fill the tensors with the pre-trained embeddings\n",
        "    for word in w2i.keys():\n",
        "        idx = w2i[word]\n",
        "        pretrained_word_embeddings[idx, :] = torch.from_numpy(word_embeddings_gensim[word])\n",
        "    for pos in p2i.keys():\n",
        "        idx = p2i[pos]\n",
        "        pretrained_pos_embeddings[idx, :] = torch.from_numpy(pos_embeddings_gensim[pos])\n",
        "\n",
        "    return w2i, p2i, l2i, pretrained_word_embeddings, pretrained_pos_embeddings\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOFs-N8WTxNv"
      },
      "source": [
        "**Model Building**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4P3sVWwTn0d"
      },
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self, w2i, p2i, pretrained_word_embeddings, pretrained_pos_embeddings,\n",
        "                 len_word_embed, len_pos_embed, len_feature_vec=20, lstm_hidden_size=400,\n",
        "                 mlp_arc_hidden_size=500, mlp_label_hidden_size=200, n_label=47):\n",
        "\n",
        "        super(Network, self).__init__()\n",
        "        self.len_word_embed = len_word_embed\n",
        "        self.len_pos_embed = len_pos_embed\n",
        "        self.len_data_vec = len_word_embed + len_pos_embed\n",
        "        self.len_feature_vec = len_feature_vec\n",
        "        self.lstm_hidden_size = lstm_hidden_size\n",
        "        self.mlp_arc_hidden_size = mlp_arc_hidden_size\n",
        "        self.mlp_label_hidden_size = mlp_label_hidden_size\n",
        "        self.n_label = n_label\n",
        "        self.w2i = w2i\n",
        "        self.p2i = p2i\n",
        "\n",
        "        # trainable parameters\n",
        "        self.word_embeddings = torch.nn.Embedding(len(pretrained_word_embeddings), len_word_embed)\n",
        "        self.word_embeddings.weight = torch.nn.Parameter(pretrained_word_embeddings)\n",
        "        self.pos_embeddings = torch.nn.Embedding(len(pretrained_pos_embeddings), len_pos_embed)\n",
        "        self.pos_embeddings.weight = torch.nn.Parameter(pretrained_pos_embeddings)\n",
        "\n",
        "        self.BiLSTM = torch.nn.LSTM(input_size=self.len_data_vec, hidden_size=self.lstm_hidden_size,\n",
        "                                    num_layers = 3, dropout=.33, bidirectional=True)\n",
        "\n",
        "        self.MLP_arc_head_layer1 = torch.nn.Linear(self.lstm_hidden_size * 2, mlp_arc_hidden_size)\n",
        "        self.MLP_arc_head_layer2 = torch.nn.Linear(mlp_arc_hidden_size, len_feature_vec)\n",
        "        self.MLP_arc_dep_layer1 = torch.nn.Linear(self.lstm_hidden_size * 2, mlp_arc_hidden_size)\n",
        "        self.MLP_arc_dep_layer2 = torch.nn.Linear(mlp_arc_hidden_size, len_feature_vec)\n",
        "\n",
        "        self.MLP_label_head_layer1 = torch.nn.Linear(self.lstm_hidden_size * 2, mlp_label_hidden_size)\n",
        "        self.MLP_label_head_layer2 = torch.nn.Linear(mlp_label_hidden_size, len_feature_vec)\n",
        "        self.MLP_label_dep_layer1 = torch.nn.Linear(self.lstm_hidden_size * 2, mlp_label_hidden_size)\n",
        "        self.MLP_label_dep_layer2 = torch.nn.Linear(mlp_label_hidden_size, len_feature_vec)\n",
        "\n",
        "        self.MLP_label_classifier_layer1 = torch.nn.Linear(self.len_feature_vec*2, self.len_feature_vec)\n",
        "        self.MLP_label_classifier_layer2 = torch.nn.Linear(self.len_feature_vec, self.n_label)\n",
        "\n",
        "        self.U_1 = nn.Parameter(torch.randn(len_feature_vec, len_feature_vec))\n",
        "        self.u_2 = nn.Parameter(torch.randn(1, len_feature_vec))\n",
        "\n",
        "        self.arc_loss = []\n",
        "        self.label_loss = []\n",
        "        self.total_loss = []\n",
        "        self.arc_loss_particular = []\n",
        "        self.label_loss_particular = []\n",
        "        self.total_loss_particular = []\n",
        "\n",
        "    def MLP_arc_head(self, r):\n",
        "        hidden = F.relu(self.MLP_arc_head_layer1(r))\n",
        "        h = self.MLP_arc_head_layer2(hidden)\n",
        "        return h\n",
        "\n",
        "    def MLP_arc_dep(self, r):\n",
        "        hidden = F.relu(self.MLP_arc_dep_layer1(r))\n",
        "        h = self.MLP_arc_dep_layer2(hidden)\n",
        "        return h\n",
        "\n",
        "    def MLP_label_head(self, r):\n",
        "        hidden = F.relu(self.MLP_label_head_layer1(r))\n",
        "        h = self.MLP_label_head_layer2(hidden)\n",
        "        return h\n",
        "\n",
        "    def MLP_label_dep(self, r):\n",
        "        hidden = F.relu(self.MLP_label_dep_layer1(r))\n",
        "        h = self.MLP_label_dep_layer2(hidden)\n",
        "        return h\n",
        "\n",
        "    def MLP_label_classifier(self, r):\n",
        "        hidden = F.relu(self.MLP_label_classifier_layer1(r))\n",
        "        h = self.MLP_label_classifier_layer2(hidden)\n",
        "        return h\n",
        "\n",
        "    def forward(self, sequence):\n",
        "        # read input\n",
        "        seq_len = len(sequence[0])\n",
        "        word_sequence = sequence[:,0]\n",
        "        pos_sequence = sequence[:,1]\n",
        "        gold_tree = sequence[:,2] if seq_len == 3 else None # if there is no gold tree given, only predict arcs, not labels\n",
        "\n",
        "        # find word embeddings given sequence of indices and construct data vector\n",
        "        word_embeddings = self.word_embeddings(word_sequence)\n",
        "        pos_embeddings = self.pos_embeddings(pos_sequence)\n",
        "        x = torch.cat((word_embeddings, pos_embeddings), 1)\n",
        "        x = x[:, None, :]  # add an empty y-dimension, because that's how LSTM takes its input\n",
        "\n",
        "        # initialise hidden state of the LSTM\n",
        "        hidden_init_1 = torch.zeros(6, 1, self.lstm_hidden_size)\n",
        "        hidden_init_2 = torch.zeros(6, 1, self.lstm_hidden_size)\n",
        "        if torch.cuda.is_available:\n",
        "            hidden_init_1 = hidden_init_1.cuda()\n",
        "            hidden_init_2 = hidden_init_2.cuda()\n",
        "        hidden = (autograd.Variable(hidden_init_1), autograd.Variable(hidden_init_2))\n",
        "\n",
        "        # embed words in their context\n",
        "        r, _ = self.BiLSTM(x, hidden)\n",
        "\n",
        "        # \"fork in the road\"; arcs\n",
        "        h_arc_head = torch.squeeze(self.MLP_arc_head(r))\n",
        "        h_arc_dep = torch.squeeze(self.MLP_arc_dep(r))\n",
        "        adj_matrix = h_arc_head @ self.U_1 @ torch.t(h_arc_dep) + h_arc_head @ torch.t(self.u_2)\n",
        "\n",
        "        pred_labels = None\n",
        "        if gold_tree is not None:\n",
        "            h_label_head = torch.squeeze(self.MLP_label_head(r))\n",
        "            h_label_dep = torch.squeeze(self.MLP_label_dep(r))\n",
        "            h_label_dep = h_label_dep[gold_tree.data]\n",
        "            arcs_to_label = torch.cat((h_label_head, h_label_dep),1)\n",
        "            pred_labels = self.MLP_label_classifier(arcs_to_label)\n",
        "        return adj_matrix , pred_labels"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPbCMhT0UYTg"
      },
      "source": [
        "**Training the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJrDvG6TUNTz"
      },
      "source": [
        "def train(show=True, save=False):\n",
        "    start = time.time()\n",
        "    filepath_dataset = '/content/sample_data/DATA/newfile.json'\n",
        "    data = json.load(open(filepath_dataset, 'r'))\n",
        "    # initialise word-embeddings (the starting point from which we'll train)\n",
        "    len_word_embed = 100\n",
        "    len_pos_embed = 20\n",
        "    w2i, p2i, l2i, pwe, ppe = pretrain_word_embeddings(data, len_word_embed, len_pos_embed)\n",
        "    network = Network(w2i, p2i, pwe, ppe, len_word_embed, len_pos_embed, n_label=len(l2i))\n",
        "    if torch.cuda.is_available():\n",
        "        network.cuda()\n",
        "    # initialise trainer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    lr = 0.002\n",
        "    weight_decay = 1e-6\n",
        "    betas = (0.9, 0.9)\n",
        "    optimizer = optim.Adam(network.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n",
        "    # messages\n",
        "    n_data = len(data)\n",
        "    if show:\n",
        "        if save:\n",
        "            print('training the model. Weights will be backed up...')\n",
        "        else:\n",
        "            print('performing a dry run...')\n",
        "        print('size of the dataset: ', n_data)\n",
        "\n",
        "    # start training\n",
        "    # an epoch is a loop over the entire dataset\n",
        "    for epoch in range(50):\n",
        "        arc_loss_per_data = []\n",
        "        label_loss_per_data = []\n",
        "        total_loss_per_data = []\n",
        "        for i in range(len(data)):\n",
        "            network.zero_grad()  # PyTorch remembers gradients. We can forget them now, because we are starting a new sentence\n",
        "            # prepare targets\n",
        "            seq_len = len(data[i]['words'])\n",
        "            gold_mat = convert_sentence_to_adjacency_matrix(data[i])\n",
        "            gold_tree = adjacency_matrix_to_tensor(gold_mat)\n",
        "            arc_target = Variable(gold_tree, requires_grad=False)\n",
        "            labels_target = torch.LongTensor(seq_len)\n",
        "            for j, word in enumerate(data[i]['words']):\n",
        "                labels_target[j] = l2i[word['deprel']]\n",
        "            labels_target = Variable(labels_target, requires_grad=False)\n",
        "            # prepare input\n",
        "            sequence = torch.LongTensor(seq_len, 3)\n",
        "            for j, word in enumerate(data[i]['words']):\n",
        "                sequence[j,0] = w2i[word['form']]\n",
        "                sequence[j,1] = p2i[word['xpostag']]\n",
        "                sequence[j,2] = gold_tree[j]\n",
        "            sequence_var = Variable(sequence)\n",
        "            # prepare GPU\n",
        "            if torch.cuda.is_available():\n",
        "                arc_target = arc_target.cuda()\n",
        "                labels_target = labels_target.cuda()\n",
        "                sequence_var = sequence_var.cuda()\n",
        "\n",
        "            # run the network\n",
        "            adj_mat, labels_pred = network(sequence_var)\n",
        "\n",
        "            # determine losses\n",
        "            arc_pred = torch.t(adj_mat)  # nn.CrossEntropyLoss() wants the classes in the second dimension\n",
        "            arc_loss = criterion(arc_pred, arc_target)\n",
        "            label_loss = criterion(labels_pred, labels_target)\n",
        "            total_loss = arc_loss + label_loss\n",
        "\n",
        "            # backprop\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            new_tensor = torch.t(F.softmax(torch.t(adj_mat))).data\n",
        "            if torch.cuda.is_available():\n",
        "                new_tensor = new_tensor.cpu()\n",
        "\n",
        "            plt.imshow(new_tensor.numpy())\n",
        "\n",
        "        # backup current parameters and write to log\n",
        "        current_date_and_time = strftime(\"%Y-%m-%d_%H:%M:%S\", gmtime())\n",
        "        if save:\n",
        "            torch.save(network, '/content/sample_data/DATA' + '/latest_weights')\n",
        "        # messages\n",
        "        if show:\n",
        "            print('-'*10)\n",
        "            print('latest backup at ', current_date_and_time)\n",
        "            # print('epoch {} loss {:.4f}'.format(epoch, network.total_loss[-1]))\n",
        "    end = time.time()\n",
        "    if show:\n",
        "        print('execution took ', end - start, ' seconds')\n",
        "        plt.show()\n",
        "    return"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AXC9fX9U-xN"
      },
      "source": [
        "def convert_sentence_to_adjacency_matrix(sentence):\n",
        "    '''\n",
        "    Input: sentence in json\n",
        "    Output: adjancency matrix (gold standard)\n",
        "    '''\n",
        "    sentence_len = len(sentence['words'])\n",
        "    # Initialize a matrix of size N x N\n",
        "    adjancency_matrix = np.zeros((sentence_len, sentence_len))\n",
        "    for word in sentence['words']:\n",
        "        word_id = int(word['id'])\n",
        "        head = int(word['head'])\n",
        "        # Ignore the root(0)-(-1) connection\n",
        "        if head == -1:\n",
        "            continue\n",
        "        adjancency_matrix[head][word_id] = 1\n",
        "    return adjancency_matrix\n",
        "\n",
        "\n",
        "def adjacency_matrix_to_tensor(matrix):\n",
        "    output = [0] * matrix.shape[0]\n",
        "    for i in range(matrix.shape[0]):\n",
        "        for j in range(matrix.shape[0]):\n",
        "            if matrix[i][j] == 1:\n",
        "                output[j] = i\n",
        "    output1 = torch.LongTensor(output)\n",
        "    return(output1)\n",
        "\n",
        "\n",
        "def get_labels():\n",
        "    path = '/content/sample_data/DATA/labels.json'\n",
        "    with open(path) as f:\n",
        "        content = f.readlines()\n",
        "    labels = {}\n",
        "    count = 1\n",
        "    for line in content:\n",
        "        temp = line.split(':')\n",
        "        if len(temp) == 2:\n",
        "            # labels.append(temp[0])\n",
        "            labels[count] = temp[0]\n",
        "            labels[temp[0]] = count\n",
        "            count += 1\n",
        "    with open(path + '.json', 'w+') as f:\n",
        "        f.write(json.dumps(labels, indent=4))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzCJsr9xVDg8",
        "outputId": "a431fae3-b417-4a58-866c-9bd5c2c910a0"
      },
      "source": [
        " train(save=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training the model. Weights will be backed up...\n",
            "size of the dataset:  1684\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:70: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "----------\n",
            "latest backup at  2021-04-30_15:18:47\n",
            "----------\n",
            "latest backup at  2021-04-30_15:19:58\n",
            "----------\n",
            "latest backup at  2021-04-30_15:21:08\n",
            "----------\n",
            "latest backup at  2021-04-30_15:22:19\n",
            "----------\n",
            "latest backup at  2021-04-30_15:23:30\n",
            "----------\n",
            "latest backup at  2021-04-30_15:24:42\n",
            "----------\n",
            "latest backup at  2021-04-30_15:25:54\n",
            "----------\n",
            "latest backup at  2021-04-30_15:27:06\n",
            "----------\n",
            "latest backup at  2021-04-30_15:28:17\n",
            "----------\n",
            "latest backup at  2021-04-30_15:29:29\n",
            "----------\n",
            "latest backup at  2021-04-30_15:30:42\n",
            "----------\n",
            "latest backup at  2021-04-30_15:31:55\n",
            "----------\n",
            "latest backup at  2021-04-30_15:33:08\n",
            "----------\n",
            "latest backup at  2021-04-30_15:34:21\n",
            "----------\n",
            "latest backup at  2021-04-30_15:35:34\n",
            "----------\n",
            "latest backup at  2021-04-30_15:36:47\n",
            "----------\n",
            "latest backup at  2021-04-30_15:38:01\n",
            "----------\n",
            "latest backup at  2021-04-30_15:39:16\n",
            "----------\n",
            "latest backup at  2021-04-30_15:40:29\n",
            "----------\n",
            "latest backup at  2021-04-30_15:41:42\n",
            "----------\n",
            "latest backup at  2021-04-30_15:42:56\n",
            "----------\n",
            "latest backup at  2021-04-30_15:44:11\n",
            "----------\n",
            "latest backup at  2021-04-30_15:45:26\n",
            "----------\n",
            "latest backup at  2021-04-30_15:46:40\n",
            "----------\n",
            "latest backup at  2021-04-30_15:47:54\n",
            "----------\n",
            "latest backup at  2021-04-30_15:49:10\n",
            "----------\n",
            "latest backup at  2021-04-30_15:50:26\n",
            "----------\n",
            "latest backup at  2021-04-30_15:51:41\n",
            "----------\n",
            "latest backup at  2021-04-30_15:52:56\n",
            "----------\n",
            "latest backup at  2021-04-30_15:54:13\n",
            "----------\n",
            "latest backup at  2021-04-30_15:55:29\n",
            "----------\n",
            "latest backup at  2021-04-30_15:56:46\n",
            "----------\n",
            "latest backup at  2021-04-30_15:58:02\n",
            "----------\n",
            "latest backup at  2021-04-30_15:59:17\n",
            "----------\n",
            "latest backup at  2021-04-30_16:00:35\n",
            "----------\n",
            "latest backup at  2021-04-30_16:01:54\n",
            "----------\n",
            "latest backup at  2021-04-30_16:03:13\n",
            "----------\n",
            "latest backup at  2021-04-30_16:04:30\n",
            "----------\n",
            "latest backup at  2021-04-30_16:05:47\n",
            "----------\n",
            "latest backup at  2021-04-30_16:07:06\n",
            "----------\n",
            "latest backup at  2021-04-30_16:08:25\n",
            "----------\n",
            "latest backup at  2021-04-30_16:09:45\n",
            "----------\n",
            "latest backup at  2021-04-30_16:11:02\n",
            "----------\n",
            "latest backup at  2021-04-30_16:12:21\n",
            "----------\n",
            "latest backup at  2021-04-30_16:13:41\n",
            "----------\n",
            "latest backup at  2021-04-30_16:15:02\n",
            "----------\n",
            "latest backup at  2021-04-30_16:16:23\n",
            "----------\n",
            "latest backup at  2021-04-30_16:17:43\n",
            "----------\n",
            "latest backup at  2021-04-30_16:19:04\n",
            "----------\n",
            "latest backup at  2021-04-30_16:20:26\n",
            "execution took  3771.6644802093506  seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuCN5qcPVGng"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}