{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import OrderedDict\n",
    "from sklearn.naive_bayes import GaussianNB,BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_sent=\"UD_Hindi-HDTB/hi_hdtb-ud-train.txt\"\n",
    "train_file_stat=\"UD_Hindi-HDTB/hi_hdtb-ud-train.conllu\" \n",
    "test_file_sent=\"UD_Hindi-HDTB/hi_hdtb-ud-test.txt\"\n",
    "test_file_stat=\"UD_Hindi-HDTB/hi_hdtb-ud-test.conllu\" \n",
    "val_file_sent=\"UD_Hindi-HDTB/hi_hdtb-ud-dev.txt\"\n",
    "val_file_stat=\"UD_Hindi-HDTB/hi_hdtb-ud-dev.conllu\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_lines(name):\n",
    "    lines=[]\n",
    "    with open(name,'r',encoding = 'utf-8') as f:\n",
    "        lines=f.readlines()\n",
    "    for i in range(0,len(lines)):\n",
    "        lines[i]=lines[i].strip().split('\\t')\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_stats(line):\n",
    "    stats=dict()\n",
    "    stats['form']=line[1]\n",
    "    stats['lemma']=line[2]\n",
    "    stats['upos']=line[3]\n",
    "    stats['xpos']=line[4]\n",
    "    stats['head']=int(line[6])\n",
    "    stats['deprel']=line[7]\n",
    "    return int(line[0]),stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_exists(head,i,graph):\n",
    "    while i!=head and i!=0:\n",
    "        i=graph[i]\n",
    "    return i==head\n",
    "def projectivity(sentence):\n",
    "    graph=dict()\n",
    "    for word in sentence:\n",
    "        word_id=word\n",
    "        head_id=sentence[word_id]['head']\n",
    "        graph[word_id]=head_id\n",
    "    for dependent in graph:\n",
    "        head=graph[dependent]\n",
    "        l= min(head,dependent)+1\n",
    "        r=max(head,dependent)\n",
    "        for i in range(l,r,1):\n",
    "            if not path_exists(head,i,graph):\n",
    "                return False            \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_non_projective(sentences):\n",
    "    filt_sentences=[sentence for sentence in sentences if projectivity(sentence)]\n",
    "    return filt_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(file):\n",
    "    lines=file_lines(file)\n",
    "    sentences=[]\n",
    "    sentence={}\n",
    "    for line in lines:\n",
    "        if len(line)<10:\n",
    "            if len(sentence)>0:\n",
    "                #do something with sentence\n",
    "                sentences.append(sentence)\n",
    "                sentence={}\n",
    "        else:\n",
    "            id,stats=extract_stats(line)\n",
    "            sentence[id]=stats\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(sentence):\n",
    "    buffer=list()\n",
    "    stack=[0]\n",
    "    arcs=list()\n",
    "    states=[]\n",
    "    transitions=[]\n",
    "    def perform_action(action,label=''):\n",
    "        nonlocal buffer,stack,arcs,states,transitions\n",
    "        states.append((list(buffer),list(stack),list(arcs)))\n",
    "        transitions.append((action,label))\n",
    "        if action=='shift':\n",
    "            stack.append(buffer.pop())\n",
    "        elif action=='left_arc':\n",
    "            arcs.append((stack[-1],stack[-2],label))\n",
    "            stack.pop(-2)\n",
    "        elif action=='right_arc':\n",
    "            arcs.append((stack[-2],stack[-1],label))\n",
    "            stack.pop()\n",
    "    dependency_graph = defaultdict(lambda: defaultdict())\n",
    "    for word in reversed(sentence): #store in reverse in buffer, so top of buffer is first word\n",
    "        buffer.append(word['id'])\n",
    "        dependency_graph[word['head']][word['id']]=word['deprel']\n",
    "    states=[]\n",
    "    transitions=[]\n",
    "    head_found=dict()\n",
    "    while not (len(stack)==1 and stack[0]==0 and len(buffer)==0):\n",
    "        if len(stack)>=2:\n",
    "            if dependency_graph.get(stack[-1]) is not None and dependency_graph[stack[-1]].get(stack[-2]) is not None:\n",
    "                head_found[stack[-2]]=True\n",
    "                perform_action(action='left_arc',label=dependency_graph[stack[-1]][stack[-2]])\n",
    "            elif dependency_graph.get(stack[-2]) is not None and dependency_graph[stack[-2]].get(stack[-1]) is not None:\n",
    "                if dependency_graph.get(stack[-1]) is not None and any([dependent not in head_found for dependent in dependency_graph[stack[-1]].keys()]):\n",
    "                    perform_action(action='shift')\n",
    "                else:\n",
    "                    head_found[stack[-1]]=True\n",
    "                    perform_action(action='right_arc',label=dependency_graph[stack[-2]][stack[-1]])\n",
    "            else:\n",
    "                perform_action(action='shift')\n",
    "        else:\n",
    "            perform_action(action='shift')\n",
    "    return states,transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_eager(sentence):\n",
    "    buffer=list()\n",
    "    stack=[0]\n",
    "    arcs={}\n",
    "    states=[]\n",
    "    transitions=[]\n",
    "    def perform_action(action,label=''):\n",
    "        nonlocal buffer,stack,arcs,states,transitions\n",
    "        states.append((list(buffer),list(stack),arcs.copy()))\n",
    "        transitions.append((action,label))\n",
    "        if action=='shift':\n",
    "            stack.append(buffer.pop())\n",
    "        elif action=='reduce':\n",
    "            stack.pop()\n",
    "        elif action=='left_arc':\n",
    "            if arcs.get(buffer[-1])==None:\n",
    "                arcs[buffer[-1]]=[(stack.pop(),label)]\n",
    "            else:\n",
    "                arcs[buffer[-1]].append((stack.pop(),label))\n",
    "        elif action=='right_arc':\n",
    "            if arcs.get(stack[-1])==None:\n",
    "                arcs[stack[-1]]=[(buffer[-1],label)]\n",
    "            else:\n",
    "                arcs[stack[-1]].append((buffer[-1],label))\n",
    "            stack.append(buffer.pop())\n",
    "    dependency_graph = defaultdict(lambda: defaultdict())\n",
    "    for word in reversed(sentence): #store in reverse in buffer, so top of buffer is first word\n",
    "        buffer.append(word)\n",
    "        dependency_graph[sentence[word]['head']][word]=sentence[word]['deprel']\n",
    "    head_found=dict()\n",
    "    while not len(buffer)==0:\n",
    "        if len(stack)>=1:\n",
    "            if stack[-1]!=0 and head_found.get(stack[-1])==None and dependency_graph.get(buffer[-1]) is not None and dependency_graph[buffer[-1]].get(stack[-1]) is not None:\n",
    "                head_found[stack[-1]]=True\n",
    "                perform_action(action='left_arc',label=dependency_graph[buffer[-1]][stack[-1]])\n",
    "            elif head_found.get(buffer[-1])==None and dependency_graph.get(stack[-1]) is not None and dependency_graph[stack[-1]].get(buffer[-1]) is not None:\n",
    "                head_found[buffer[-1]]=True\n",
    "                perform_action(action='right_arc',label=dependency_graph[stack[-1]][buffer[-1]])\n",
    "            elif head_found.get(stack[-1])!=None:\n",
    "                perform_action(action='reduce')\n",
    "            else:\n",
    "                perform_action(action='shift')\n",
    "        else:\n",
    "            perform_action(action='shift')\n",
    "    return states,transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_names(feature_array):\n",
    "    feature_names=[]\n",
    "    if 'single_words' in feature_array:\n",
    "        feature_names.extend(['S0_w_p','S0_w','S0_p','N0_w_p','N0_w','N0_p','N1_w_p','N1_w','N1_p','N2_w_p','N2_w','N2_p'])\n",
    "    return feature_names\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_oneword(state,row,sentence):\n",
    "    buffer,stack,arcs=state\n",
    "    if len(stack)>1:\n",
    "        row['S0_w']=sentence[stack[-1]]['form']\n",
    "        row['S0_p']=sentence[stack[-1]]['xpos']\n",
    "        row['S0_w_p']=sentence[stack[-1]]['form']+'_'+sentence[stack[-1]]['xpos']\n",
    "    else:\n",
    "        row['S0_w']='None'\n",
    "        row['S0_p']='None'\n",
    "        row['S0_w_p']='None'\n",
    "    if len(buffer)>=1:\n",
    "        row['N0_w']=sentence[buffer[-1]]['form']\n",
    "        row['N0_p']=sentence[buffer[-1]]['xpos']\n",
    "        row['N0_w_p']=sentence[buffer[-1]]['form']+'_'+sentence[buffer[-1]]['xpos']\n",
    "    else:\n",
    "        row['N0_w']='None'\n",
    "        row['N0_p']='None'\n",
    "        row['N0_w_p']='None'\n",
    "    if len(buffer)>=2:\n",
    "        row['N1_w']=sentence[buffer[-2]]['form']\n",
    "        row['N1_p']=sentence[buffer[-2]]['xpos']\n",
    "        row['N1_w_p']=sentence[buffer[-2]]['form']+'_'+sentence[buffer[-2]]['xpos']\n",
    "    else:\n",
    "        row['N1_w']='None'\n",
    "        row['N1_p']='None'\n",
    "        row['N1_w_p']='None'\n",
    "    if len(buffer)>=3:\n",
    "        row['N2_w']=sentence[buffer[-3]]['form']\n",
    "        row['N2_p']=sentence[buffer[-3]]['xpos']\n",
    "        row['N2_w_p']=sentence[buffer[-3]]['form']+'_'+sentence[buffer[-3]]['xpos']\n",
    "    else:\n",
    "        row['N2_w']='None'\n",
    "        row['N2_p']='None'\n",
    "        row['N2_w_p']='None'\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_features(states,transitions,sentence,data,counter,label):\n",
    "    for state,transition in zip(states,transitions):\n",
    "        row={}\n",
    "        row=extract_features_oneword(state,row,sentence)\n",
    "        if transition[1]=='':\n",
    "            label[counter]=transition[0]\n",
    "        else:\n",
    "            label[counter]=transition[0]+','+transition[1]\n",
    "        data[counter]=row\n",
    "        counter+=1\n",
    "    return counter\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_features_old(states,transitions,feature_names,sentence,data,counter,label=-1):\n",
    "    for state,transition in zip(states,transitions):\n",
    "        buffer,stack,arcs=state\n",
    "        row={}\n",
    "        for feature_name in feature_names[:-1]:\n",
    "            feature_tokens=feature_name.split('_')\n",
    "            cur_word=0\n",
    "            val=''\n",
    "            for token in feature_tokens:\n",
    "                \n",
    "                #determining word\n",
    "                if token[0]=='S':\n",
    "                    if int(token[1])<len(stack):\n",
    "                        cur_word=stack[-1-int(token[1])]\n",
    "                        if  cur_word==0:\n",
    "                            row[feature_name]='None'\n",
    "                            break\n",
    "                    else:\n",
    "                        row[feature_name]='None'\n",
    "                        break\n",
    "                elif token[0]=='N':\n",
    "                    if int(token[1])<len(buffer):\n",
    "                        cur_word=buffer[-1-int(token[1])]\n",
    "                        if  cur_word==0:\n",
    "                            row[feature_name]='None'\n",
    "                            break\n",
    "                    else:\n",
    "                        row[feature_name]='None'\n",
    "                        break\n",
    "                \n",
    "                #determining feature of the word\n",
    "                else:\n",
    "                    if token=='w':\n",
    "                        if len(val)==0:\n",
    "                            val=sentence[cur_word]['form']\n",
    "                            break\n",
    "                        else:\n",
    "                            val=val+'_'+sentence[cur_word]['form']\n",
    "                            break\n",
    "                    elif token=='p':\n",
    "                        if len(val)==0:\n",
    "                            val=sentence[cur_word]['xpos']\n",
    "                            break\n",
    "                        else:\n",
    "                            val=val+'_'+sentence[cur_word]['xpos']\n",
    "                            break\n",
    "                           \n",
    "            row[feature_name]=val\n",
    "        if transition[1]=='':\n",
    "            row['transition']=transition[0]\n",
    "        else:\n",
    "            row['transition']=transition[0]+','+transition[1]\n",
    "            \n",
    "        data[counter]=row\n",
    "        counter+=1\n",
    "    return counter\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_eager_test(sentence,model,enc):\n",
    "    buffer=list()\n",
    "    stack=[0]\n",
    "    arcs={}\n",
    "    states=[]\n",
    "    transitions=[]\n",
    "    def perform_action(action,label=''):\n",
    "        nonlocal buffer,stack,arcs,states,transitions\n",
    "        states.append((list(buffer),list(stack),arcs.copy()))\n",
    "        transitions.append((action,label))\n",
    "        if action=='shift':\n",
    "            stack.append(buffer.pop())\n",
    "        elif action=='reduce':\n",
    "            stack.pop()\n",
    "        elif action=='left_arc':\n",
    "            if arcs.get(buffer[-1])==None:\n",
    "                arcs[buffer[-1]]=[(stack.pop(),label)]\n",
    "            else:\n",
    "                arcs[buffer[-1]].append((stack.pop(),label))\n",
    "        elif action=='right_arc':\n",
    "            if arcs.get(stack[-1])==None:\n",
    "                arcs[stack[-1]]=[(buffer[-1],label)]\n",
    "            else:\n",
    "                arcs[stack[-1]].append((buffer[-1],label))\n",
    "            stack.append(buffer.pop())\n",
    "    for word in reversed(sentence): #store in reverse in buffer, so top of buffer is first word\n",
    "        buffer.append(word)\n",
    "    head_found=dict()\n",
    "    while not len(buffer)==0:\n",
    "        state=(list(buffer),list(stack),arcs.copy())\n",
    "        row={}\n",
    "        extract_features_oneword(state,row,sentence)\n",
    "        data={0:row}\n",
    "        df = pd.DataFrame.from_dict(data,'index')\n",
    "        x=enc.transform(df)\n",
    "        decision_probs=np.argsort(-1 *model.predict_proba(x)[0])\n",
    "        print(state)\n",
    "        for decision in decision_probs:\n",
    "            temp=model.classes_[decision].split(',')\n",
    "            print(temp)\n",
    "            transition=temp[0]\n",
    "            label=temp[1] if len(temp)>1 else ''\n",
    "            if transition=='left_arc':\n",
    "                if stack[-1]!=0 and head_found.get(stack[-1])==None and len(stack)>=1:\n",
    "                    head_found[stack[-1]]=buffer[-1]\n",
    "                    perform_action(action='left_arc',label=label)\n",
    "                    break\n",
    "            if transition=='right_arc':\n",
    "                if len(stack)>=1 and head_found.get(buffer[-1])==None:\n",
    "                    head_found[buffer[-1]]=stack[-1]\n",
    "                    perform_action(action='right_arc',label=label) \n",
    "                    break\n",
    "            if transition=='reduce':\n",
    "                if head_found.get(stack[-1])!=None and len(stack)>=1:\n",
    "                    perform_action(action='reduce')\n",
    "                    break\n",
    "            if (transition=='shift' and stack[-1] in arcs) or decision==decision_probs[-1]:\n",
    "                perform_action(action='shift')\n",
    "                break\n",
    "    return head_found,states,transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    sentences=get_sentences(train_file_stat)\n",
    "    filt_sentences=filter_non_projective(sentences)\n",
    "    data={}\n",
    "    label={}\n",
    "    counter=0\n",
    "    for i in tqdm(range(len(filt_sentences))):\n",
    "        states,transitions=parse_eager(filt_sentences[i])\n",
    "        counter=extract_features(states,transitions,filt_sentences[i],data,counter,label)\n",
    "    df = pd.DataFrame.from_dict(data,'index')\n",
    "    label=np.array(list(label.values()))\n",
    "    enc = OneHotEncoder(handle_unknown='ignore',sparse=True,dtype=int)\n",
    "    x=enc.fit_transform(df)\n",
    "    model= BernoulliNB()\n",
    "    model.fit(x,label)\n",
    "    return model,enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a476211123c4ea09eee37e68ace4a31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11467.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model,enc=get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=get_sentences(test_file_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_sentences=filter_non_projective(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1], [0], {})\n",
      "['shift']\n",
      "['right_arc', 'root']\n",
      "([14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2], [0, 1], {0: [(1, 'root')]})\n",
      "['right_arc', 'case']\n",
      "([14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3], [0, 1, 2], {0: [(1, 'root')], 1: [(2, 'case')]})\n",
      "['reduce']\n",
      "([14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3], [0, 1], {0: [(1, 'root')], 1: [(2, 'case')]})\n",
      "['shift']\n",
      "([14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4], [0, 1, 3], {0: [(1, 'root')], 1: [(2, 'case')]})\n",
      "['left_arc', 'compound']\n",
      "([14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4], [0, 1], {0: [(1, 'root')], 1: [(2, 'case')], 4: [(3, 'compound')]})\n",
      "['shift']\n",
      "([14, 13, 12, 11, 10, 9, 8, 7, 6, 5], [0, 1, 4], {0: [(1, 'root')], 1: [(2, 'case')], 4: [(3, 'compound')]})\n",
      "['shift']\n",
      "([14, 13, 12, 11, 10, 9, 8, 7, 6], [0, 1, 4, 5], {0: [(1, 'root')], 1: [(2, 'case')], 4: [(3, 'compound')]})\n",
      "['shift']\n",
      "['reduce']\n",
      "['right_arc', 'case']\n",
      "([14, 13, 12, 11, 10, 9, 8, 7], [0, 1, 4, 5, 6], {0: [(1, 'root')], 1: [(2, 'case')], 4: [(3, 'compound')], 5: [(6, 'case')]})\n",
      "['left_arc', 'compound']\n",
      "['shift']\n",
      "['left_arc', 'nmod']\n",
      "['reduce']\n",
      "([14, 13, 12, 11, 10, 9, 8, 7], [0, 1, 4, 5], {0: [(1, 'root')], 1: [(2, 'case')], 4: [(3, 'compound')], 5: [(6, 'case')]})\n",
      "['shift']\n",
      "([14, 13, 12, 11, 10, 9, 8], [0, 1, 4, 5, 7], {0: [(1, 'root')], 1: [(2, 'case')], 4: [(3, 'compound')], 5: [(6, 'case')]})\n",
      "['shift']\n",
      "['reduce']\n",
      "['right_arc', 'case']\n",
      "([14, 13, 12, 11, 10, 9], [0, 1, 4, 5, 7, 8], {0: [(1, 'root')], 1: [(2, 'case')], 4: [(3, 'compound')], 5: [(6, 'case')], 7: [(8, 'case')]})\n",
      "['shift']\n",
      "['left_arc', 'cc']\n",
      "['reduce']\n",
      "([14, 13, 12, 11, 10, 9], [0, 1, 4, 5, 7], {0: [(1, 'root')], 1: [(2, 'case')], 4: [(3, 'compound')], 5: [(6, 'case')], 7: [(8, 'case')]})\n",
      "['left_arc', 'nmod']\n",
      "([14, 13, 12, 11, 10, 9], [0, 1, 4, 5], {0: [(1, 'root')], 1: [(2, 'case')], 4: [(3, 'compound')], 5: [(6, 'case')], 7: [(8, 'case')], 9: [(7, 'nmod')]})\n",
      "['shift']\n",
      "([14, 13, 12, 11, 10], [0, 1, 4, 5, 9], {0: [(1, 'root')], 1: [(2, 'case')], 4: [(3, 'compound')], 5: [(6, 'case')], 7: [(8, 'case')], 9: [(7, 'nmod')]})\n",
      "['reduce']\n",
      "['shift']\n",
      "([14, 13, 12, 11], [0, 1, 4, 5, 9, 10], {0: [(1, 'root')], 1: [(2, 'case')], 4: [(3, 'compound')], 5: [(6, 'case')], 7: [(8, 'case')], 9: [(7, 'nmod')]})\n",
      "['reduce']\n",
      "['shift']\n",
      "['left_arc', 'nmod']\n",
      "([14, 13, 12, 11], [0, 1, 4, 5, 9], {0: [(1, 'root')], 1: [(2, 'case')], 4: [(3, 'compound')], 5: [(6, 'case')], 7: [(8, 'case')], 9: [(7, 'nmod')], 11: [(10, 'nmod')]})\n",
      "['shift']\n",
      "([14, 13, 12], [0, 1, 4, 5, 9, 11], {0: [(1, 'root')], 1: [(2, 'case')], 4: [(3, 'compound')], 5: [(6, 'case')], 7: [(8, 'case')], 9: [(7, 'nmod')], 11: [(10, 'nmod')]})\n",
      "['left_arc', 'amod']\n",
      "([14, 13, 12], [0, 1, 4, 5, 9], {0: [(1, 'root')], 1: [(2, 'case')], 4: [(3, 'compound')], 5: [(6, 'case')], 7: [(8, 'case')], 9: [(7, 'nmod')], 11: [(10, 'nmod')], 12: [(11, 'amod')]})\n",
      "['left_arc', 'nmod']\n",
      "([14, 13, 12], [0, 1, 4, 5], {0: [(1, 'root')], 1: [(2, 'case')], 4: [(3, 'compound')], 5: [(6, 'case')], 7: [(8, 'case')], 9: [(7, 'nmod')], 11: [(10, 'nmod')], 12: [(11, 'amod'), (9, 'nmod')]})\n",
      "['shift']\n",
      "([14, 13], [0, 1, 4, 5, 12], {0: [(1, 'root')], 1: [(2, 'case')], 4: [(3, 'compound')], 5: [(6, 'case')], 7: [(8, 'case')], 9: [(7, 'nmod')], 11: [(10, 'nmod')], 12: [(11, 'amod'), (9, 'nmod')]})\n",
      "['left_arc', 'obl']\n",
      "([14, 13], [0, 1, 4, 5], {0: [(1, 'root')], 1: [(2, 'case')], 4: [(3, 'compound')], 5: [(6, 'case')], 7: [(8, 'case')], 9: [(7, 'nmod')], 11: [(10, 'nmod')], 12: [(11, 'amod'), (9, 'nmod')], 13: [(12, 'obl')]})\n",
      "['reduce']\n",
      "['shift']\n",
      "([14], [0, 1, 4, 5, 13], {0: [(1, 'root')], 1: [(2, 'case')], 4: [(3, 'compound')], 5: [(6, 'case')], 7: [(8, 'case')], 9: [(7, 'nmod')], 11: [(10, 'nmod')], 12: [(11, 'amod'), (9, 'nmod')], 13: [(12, 'obl')]})\n",
      "['right_arc', 'punct']\n"
     ]
    }
   ],
   "source": [
    "heads,states,transitions=parse_eager_test(filt_sentences[0],model,enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0, 2: 1, 3: 4, 6: 5, 8: 7, 7: 9, 10: 11, 11: 12, 9: 12, 12: 13, 14: 13}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1], [0], {}),\n",
       " ([14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2], [0, 1], {}),\n",
       " ([14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3], [0, 1, 2], {1: [(2, 'case')]}),\n",
       " ([14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3], [0, 1], {1: [(2, 'case')]}),\n",
       " ([14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4], [0, 1, 3], {1: [(2, 'case')]}),\n",
       " ([14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4],\n",
       "  [0, 1],\n",
       "  {1: [(2, 'case')], 4: [(3, 'compound')]}),\n",
       " ([14, 13, 12, 11, 10, 9, 8, 7, 6, 5],\n",
       "  [0, 1, 4],\n",
       "  {1: [(2, 'case')], 4: [(3, 'compound')]}),\n",
       " ([14, 13, 12, 11, 10, 9, 8, 7, 6],\n",
       "  [0, 1, 4, 5],\n",
       "  {1: [(2, 'case')], 4: [(3, 'compound')]}),\n",
       " ([14, 13, 12, 11, 10, 9, 8, 7],\n",
       "  [0, 1, 4, 5, 6],\n",
       "  {1: [(2, 'case')], 4: [(3, 'compound')], 5: [(6, 'case')]}),\n",
       " ([14, 13, 12, 11, 10, 9, 8, 7],\n",
       "  [0, 1, 4, 5],\n",
       "  {1: [(2, 'case')], 4: [(3, 'compound')], 5: [(6, 'case')]}),\n",
       " ([14, 13, 12, 11, 10, 9, 8],\n",
       "  [0, 1, 4, 5, 7],\n",
       "  {1: [(2, 'case')], 4: [(3, 'compound')], 5: [(6, 'case')]}),\n",
       " ([14, 13, 12, 11, 10, 9],\n",
       "  [0, 1, 4, 5, 7, 8],\n",
       "  {1: [(2, 'case')],\n",
       "   4: [(3, 'compound')],\n",
       "   5: [(6, 'case')],\n",
       "   7: [(8, 'case')]}),\n",
       " ([14, 13, 12, 11, 10, 9],\n",
       "  [0, 1, 4, 5, 7],\n",
       "  {1: [(2, 'case')],\n",
       "   4: [(3, 'compound')],\n",
       "   5: [(6, 'case')],\n",
       "   7: [(8, 'case')]}),\n",
       " ([14, 13, 12, 11, 10, 9],\n",
       "  [0, 1, 4, 5],\n",
       "  {1: [(2, 'case')],\n",
       "   4: [(3, 'compound')],\n",
       "   5: [(6, 'case')],\n",
       "   7: [(8, 'case')],\n",
       "   9: [(7, 'nmod')]}),\n",
       " ([14, 13, 12, 11, 10],\n",
       "  [0, 1, 4, 5, 9],\n",
       "  {1: [(2, 'case')],\n",
       "   4: [(3, 'compound')],\n",
       "   5: [(6, 'case')],\n",
       "   7: [(8, 'case')],\n",
       "   9: [(7, 'nmod')]}),\n",
       " ([14, 13, 12, 11],\n",
       "  [0, 1, 4, 5, 9, 10],\n",
       "  {1: [(2, 'case')],\n",
       "   4: [(3, 'compound')],\n",
       "   5: [(6, 'case')],\n",
       "   7: [(8, 'case')],\n",
       "   9: [(7, 'nmod')]}),\n",
       " ([14, 13, 12, 11],\n",
       "  [0, 1, 4, 5, 9],\n",
       "  {1: [(2, 'case')],\n",
       "   4: [(3, 'compound')],\n",
       "   5: [(6, 'case')],\n",
       "   7: [(8, 'case')],\n",
       "   9: [(7, 'nmod')],\n",
       "   11: [(10, 'nmod')]}),\n",
       " ([14, 13, 12],\n",
       "  [0, 1, 4, 5, 9, 11],\n",
       "  {1: [(2, 'case')],\n",
       "   4: [(3, 'compound')],\n",
       "   5: [(6, 'case')],\n",
       "   7: [(8, 'case')],\n",
       "   9: [(7, 'nmod')],\n",
       "   11: [(10, 'nmod')]}),\n",
       " ([14, 13, 12],\n",
       "  [0, 1, 4, 5, 9],\n",
       "  {1: [(2, 'case')],\n",
       "   4: [(3, 'compound')],\n",
       "   5: [(6, 'case')],\n",
       "   7: [(8, 'case')],\n",
       "   9: [(7, 'nmod')],\n",
       "   11: [(10, 'nmod')],\n",
       "   12: [(11, 'amod'), (9, 'nmod')]}),\n",
       " ([14, 13, 12],\n",
       "  [0, 1, 4, 5],\n",
       "  {1: [(2, 'case')],\n",
       "   4: [(3, 'compound')],\n",
       "   5: [(6, 'case')],\n",
       "   7: [(8, 'case')],\n",
       "   9: [(7, 'nmod')],\n",
       "   11: [(10, 'nmod')],\n",
       "   12: [(11, 'amod'), (9, 'nmod')]}),\n",
       " ([14, 13],\n",
       "  [0, 1, 4, 5, 12],\n",
       "  {1: [(2, 'case')],\n",
       "   4: [(3, 'compound')],\n",
       "   5: [(6, 'case')],\n",
       "   7: [(8, 'case')],\n",
       "   9: [(7, 'nmod')],\n",
       "   11: [(10, 'nmod')],\n",
       "   12: [(11, 'amod'), (9, 'nmod')]}),\n",
       " ([14, 13],\n",
       "  [0, 1, 4, 5],\n",
       "  {1: [(2, 'case')],\n",
       "   4: [(3, 'compound')],\n",
       "   5: [(6, 'case')],\n",
       "   7: [(8, 'case')],\n",
       "   9: [(7, 'nmod')],\n",
       "   11: [(10, 'nmod')],\n",
       "   12: [(11, 'amod'), (9, 'nmod')],\n",
       "   13: [(12, 'obl'), (14, 'punct')]}),\n",
       " ([14],\n",
       "  [0, 1, 4, 5, 13],\n",
       "  {1: [(2, 'case')],\n",
       "   4: [(3, 'compound')],\n",
       "   5: [(6, 'case')],\n",
       "   7: [(8, 'case')],\n",
       "   9: [(7, 'nmod')],\n",
       "   11: [(10, 'nmod')],\n",
       "   12: [(11, 'amod'), (9, 'nmod')],\n",
       "   13: [(12, 'obl'), (14, 'punct')]})]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'form': 'इसके',\n",
       "  'lemma': 'यह',\n",
       "  'upos': 'PRON',\n",
       "  'xpos': 'PRP',\n",
       "  'head': 12,\n",
       "  'deprel': 'nmod'},\n",
       " 2: {'form': 'अतिरिक्त',\n",
       "  'lemma': 'अतिरिक्त',\n",
       "  'upos': 'ADP',\n",
       "  'xpos': 'PSP',\n",
       "  'head': 1,\n",
       "  'deprel': 'case'},\n",
       " 3: {'form': 'गुग्गुल',\n",
       "  'lemma': 'गुग्गुल',\n",
       "  'upos': 'PROPN',\n",
       "  'xpos': 'NNPC',\n",
       "  'head': 4,\n",
       "  'deprel': 'compound'},\n",
       " 4: {'form': 'कुंड',\n",
       "  'lemma': 'कुंड',\n",
       "  'upos': 'PROPN',\n",
       "  'xpos': 'NNP',\n",
       "  'head': 12,\n",
       "  'deprel': 'nsubj'},\n",
       " 5: {'form': ',',\n",
       "  'lemma': 'COMMA',\n",
       "  'upos': 'PUNCT',\n",
       "  'xpos': 'SYM',\n",
       "  'head': 7,\n",
       "  'deprel': 'punct'},\n",
       " 6: {'form': 'भीम',\n",
       "  'lemma': 'भीम',\n",
       "  'upos': 'PROPN',\n",
       "  'xpos': 'NNPC',\n",
       "  'head': 7,\n",
       "  'deprel': 'compound'},\n",
       " 7: {'form': 'गुफा',\n",
       "  'lemma': 'गुफा',\n",
       "  'upos': 'PROPN',\n",
       "  'xpos': 'NNP',\n",
       "  'head': 4,\n",
       "  'deprel': 'conj'},\n",
       " 8: {'form': 'तथा',\n",
       "  'lemma': 'तथा',\n",
       "  'upos': 'CCONJ',\n",
       "  'xpos': 'CC',\n",
       "  'head': 9,\n",
       "  'deprel': 'cc'},\n",
       " 9: {'form': 'भीमशिला',\n",
       "  'lemma': 'भीमशिला',\n",
       "  'upos': 'PROPN',\n",
       "  'xpos': 'NNP',\n",
       "  'head': 4,\n",
       "  'deprel': 'conj'},\n",
       " 10: {'form': 'भी',\n",
       "  'lemma': 'भी',\n",
       "  'upos': 'PART',\n",
       "  'xpos': 'RP',\n",
       "  'head': 9,\n",
       "  'deprel': 'dep'},\n",
       " 11: {'form': 'दर्शनीय',\n",
       "  'lemma': 'दर्शनीय',\n",
       "  'upos': 'ADJ',\n",
       "  'xpos': 'JJ',\n",
       "  'head': 12,\n",
       "  'deprel': 'amod'},\n",
       " 12: {'form': 'स्थल',\n",
       "  'lemma': 'स्थल',\n",
       "  'upos': 'NOUN',\n",
       "  'xpos': 'NN',\n",
       "  'head': 0,\n",
       "  'deprel': 'root'},\n",
       " 13: {'form': 'हैं',\n",
       "  'lemma': 'है',\n",
       "  'upos': 'AUX',\n",
       "  'xpos': 'VM',\n",
       "  'head': 12,\n",
       "  'deprel': 'cop'},\n",
       " 14: {'form': '।',\n",
       "  'lemma': '।',\n",
       "  'upos': 'PUNCT',\n",
       "  'xpos': 'SYM',\n",
       "  'head': 12,\n",
       "  'deprel': 'punct'}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filt_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_sentences=filter_non_projective(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filt_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names=get_feature_names(['single_words'])\n",
    "feature_names.append('transition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data={}\n",
    "label={}\n",
    "counter=0\n",
    "for i in tqdm(range(len(filt_sentences))):\n",
    "    states,transitions=parse_eager(filt_sentences[i])\n",
    "    counter=extract_features_oneword(states,transitions,filt_sentences[i],data,counter,label)\n",
    "    #counter=extract_features(states,transitions,feature_names,filt_sentences[i],data,counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(data,'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label=np.array(list(label.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore',sparse=True,dtype=int)\n",
    "x=enc.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model=model = BernoulliNB()\n",
    "model.fit(x,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states,transitions=parse_eager(sentences[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=extract_features_oneword(states,transitions,sentences[0])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer,stack,arcs=states[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(states)):\n",
    "    print('buffer:\\t',states[i][0])\n",
    "    print('stack:\\t',states[i][1])\n",
    "    print('arcs:\\t',states[i][2])\n",
    "    print('transition:\\t',transitions[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "a=['1','23','e']\n",
    "b=['d','232','wewe']\n",
    "for x,y in zip(a,b):\n",
    "    print(x)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_something(a,counter):\n",
    "    a[4]='dsds'\n",
    "    counter=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a={1:'dsa',2:'asd',3:'d'}\n",
    "counter=1\n",
    "do_something(a,counter)\n",
    "print(a)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=get_sentences(train_file_stat)\n",
    "filt_sentences=filter_non_projective(sentences)\n",
    "data={}\n",
    "label={}\n",
    "counter=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "states,transitions=parse_eager(filt_sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([7, 6, 5, 4, 3, 2, 1], [0], {}),\n",
       " ([7, 6, 5, 4, 3, 2], [0, 1], {}),\n",
       " ([7, 6, 5, 4, 3], [0, 1, 2], {}),\n",
       " ([7, 6, 5, 4, 3], [0, 1], {3: [(2, 'compound'), (4, 'case')]}),\n",
       " ([7, 6, 5, 4], [0, 1, 3], {3: [(2, 'compound'), (4, 'case')]}),\n",
       " ([7, 6, 5], [0, 1, 3, 4], {3: [(2, 'compound'), (4, 'case')]}),\n",
       " ([7, 6, 5], [0, 1, 3], {3: [(2, 'compound'), (4, 'case')]}),\n",
       " ([7, 6, 5],\n",
       "  [0, 1],\n",
       "  {3: [(2, 'compound'), (4, 'case')],\n",
       "   5: [(3, 'nsubj'), (1, 'obj'), (6, 'aux'), (7, 'punct')]}),\n",
       " ([7, 6, 5],\n",
       "  [0],\n",
       "  {3: [(2, 'compound'), (4, 'case')],\n",
       "   5: [(3, 'nsubj'), (1, 'obj'), (6, 'aux'), (7, 'punct')]}),\n",
       " ([7, 6],\n",
       "  [0, 5],\n",
       "  {3: [(2, 'compound'), (4, 'case')],\n",
       "   5: [(3, 'nsubj'), (1, 'obj'), (6, 'aux'), (7, 'punct')]}),\n",
       " ([7],\n",
       "  [0, 5, 6],\n",
       "  {3: [(2, 'compound'), (4, 'case')],\n",
       "   5: [(3, 'nsubj'), (1, 'obj'), (6, 'aux'), (7, 'punct')]}),\n",
       " ([7],\n",
       "  [0, 5],\n",
       "  {3: [(2, 'compound'), (4, 'case')],\n",
       "   5: [(3, 'nsubj'), (1, 'obj'), (6, 'aux'), (7, 'punct')]})]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'form': 'इसे',\n",
       "  'lemma': 'यह',\n",
       "  'upos': 'PRON',\n",
       "  'xpos': 'PRP',\n",
       "  'head': 5,\n",
       "  'deprel': 'obj'},\n",
       " 2: {'form': 'नवाब',\n",
       "  'lemma': 'नवाब',\n",
       "  'upos': 'NOUN',\n",
       "  'xpos': 'NNC',\n",
       "  'head': 3,\n",
       "  'deprel': 'compound'},\n",
       " 3: {'form': 'शाहजेहन',\n",
       "  'lemma': 'शाहजेहन',\n",
       "  'upos': 'PROPN',\n",
       "  'xpos': 'NNP',\n",
       "  'head': 5,\n",
       "  'deprel': 'nsubj'},\n",
       " 4: {'form': 'ने',\n",
       "  'lemma': 'ने',\n",
       "  'upos': 'ADP',\n",
       "  'xpos': 'PSP',\n",
       "  'head': 3,\n",
       "  'deprel': 'case'},\n",
       " 5: {'form': 'बनवाया',\n",
       "  'lemma': 'बनवा',\n",
       "  'upos': 'VERB',\n",
       "  'xpos': 'VM',\n",
       "  'head': 0,\n",
       "  'deprel': 'root'},\n",
       " 6: {'form': 'था',\n",
       "  'lemma': 'था',\n",
       "  'upos': 'AUX',\n",
       "  'xpos': 'VAUX',\n",
       "  'head': 5,\n",
       "  'deprel': 'aux'},\n",
       " 7: {'form': '।',\n",
       "  'lemma': '।',\n",
       "  'upos': 'PUNCT',\n",
       "  'xpos': 'SYM',\n",
       "  'head': 5,\n",
       "  'deprel': 'punct'}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filt_sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
