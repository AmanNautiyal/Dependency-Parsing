{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Transition based dependency Parsing<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import OrderedDict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_sent=\"UD_Hindi-HDTB/hi_hdtb-ud-train.txt\"\n",
    "train_file_stat=\"UD_Hindi-HDTB/hi_hdtb-ud-train.conllu\" \n",
    "test_file_sent=\"UD_Hindi-HDTB/hi_hdtb-ud-test.txt\"\n",
    "test_file_stat=\"UD_Hindi-HDTB/hi_hdtb-ud-test.conllu\" \n",
    "val_file_sent=\"UD_Hindi-HDTB/hi_hdtb-ud-dev.txt\"\n",
    "val_file_stat=\"UD_Hindi-HDTB/hi_hdtb-ud-dev.conllu\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_lines(name):\n",
    "    lines=[]\n",
    "    with open(name,'r',encoding = 'utf-8') as f:\n",
    "        lines=f.readlines()\n",
    "    for i in range(0,len(lines)):\n",
    "        lines[i]=lines[i].strip().split('\\t')\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_stats(line):\n",
    "    stats=dict()\n",
    "    stats['form']=line[1]\n",
    "    stats['lemma']=line[2]\n",
    "    stats['upos']=line[3]\n",
    "    stats['xpos']=line[4]\n",
    "    stats['head']=int(line[6])\n",
    "    stats['deprel']=line[7]\n",
    "    return int(line[0]),stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for projectivity and filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_exists(head,i,graph):\n",
    "    while i!=head and i!=0:\n",
    "        i=graph[i]\n",
    "    return i==head\n",
    "def projectivity(sentence):\n",
    "    graph=dict()\n",
    "    for word in sentence:\n",
    "        word_id=word\n",
    "        head_id=sentence[word_id]['head']\n",
    "        graph[word_id]=head_id\n",
    "    for dependent in graph:\n",
    "        head=graph[dependent]\n",
    "        l= min(head,dependent)+1\n",
    "        r=max(head,dependent)\n",
    "        for i in range(l,r,1):\n",
    "            if not path_exists(head,i,graph):\n",
    "                return False            \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_non_projective(sentences):\n",
    "    filt_sentences=[sentence for sentence in sentences if projectivity(sentence)]\n",
    "    return filt_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(file):\n",
    "    lines=file_lines(file)\n",
    "    sentences=[]\n",
    "    sentence={}\n",
    "    for line in lines:\n",
    "        if len(line)<10:\n",
    "            if len(sentence)>0:\n",
    "                #do something with sentence\n",
    "                sentences.append(sentence)\n",
    "                sentence={}\n",
    "        else:\n",
    "            id,stats=extract_stats(line)\n",
    "            sentence[id]=stats\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The arc standard parsing algorithm (not used in project)<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_standard(sentence):\n",
    "    buffer=list()\n",
    "    stack=[0]\n",
    "    arcs=list()\n",
    "    states=[]\n",
    "    transitions=[]\n",
    "    def perform_action(action,label=''):\n",
    "        nonlocal buffer,stack,arcs,states,transitions\n",
    "        states.append((list(buffer),list(stack),list(arcs)))\n",
    "        transitions.append((action,label))\n",
    "        if action=='shift':\n",
    "            stack.append(buffer.pop())\n",
    "        elif action=='left_arc':\n",
    "            arcs.append((stack[-1],stack[-2],label))\n",
    "            stack.pop(-2)\n",
    "        elif action=='right_arc':\n",
    "            arcs.append((stack[-2],stack[-1],label))\n",
    "            stack.pop()\n",
    "    dependency_graph = defaultdict(lambda: defaultdict())\n",
    "    for word in reversed(sentence): #store in reverse in buffer, so top of buffer is first word\n",
    "        buffer.append(word['id'])\n",
    "        dependency_graph[word['head']][word['id']]=word['deprel']\n",
    "    states=[]\n",
    "    transitions=[]\n",
    "    head_found=dict()\n",
    "    while not (len(stack)==1 and stack[0]==0 and len(buffer)==0):\n",
    "        if len(stack)>=2:\n",
    "            if dependency_graph.get(stack[-1]) is not None and dependency_graph[stack[-1]].get(stack[-2]) is not None:\n",
    "                head_found[stack[-2]]=True\n",
    "                perform_action(action='left_arc',label=dependency_graph[stack[-1]][stack[-2]])\n",
    "            elif dependency_graph.get(stack[-2]) is not None and dependency_graph[stack[-2]].get(stack[-1]) is not None:\n",
    "                if dependency_graph.get(stack[-1]) is not None and any([dependent not in head_found for dependent in dependency_graph[stack[-1]].keys()]):\n",
    "                    perform_action(action='shift')\n",
    "                else:\n",
    "                    head_found[stack[-1]]=True\n",
    "                    perform_action(action='right_arc',label=dependency_graph[stack[-2]][stack[-1]])\n",
    "            else:\n",
    "                perform_action(action='shift')\n",
    "        else:\n",
    "            perform_action(action='shift')\n",
    "    return states,transitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The arc standard parsing approach<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_eager(sentence):\n",
    "    buffer=list()\n",
    "    stack=[0]\n",
    "    arcs={}\n",
    "    states=[]\n",
    "    transitions=[]\n",
    "    def perform_action(action,label=''):\n",
    "        nonlocal buffer,stack,arcs,states,transitions\n",
    "        states.append((list(buffer),list(stack),deepcopy(arcs)))\n",
    "        transitions.append((action,label))\n",
    "        if action=='shift':\n",
    "            stack.append(buffer.pop())\n",
    "        elif action=='reduce':\n",
    "            stack.pop()\n",
    "        elif action=='left_arc':\n",
    "            if arcs.get(buffer[-1])==None:\n",
    "                arcs[buffer[-1]]=[(stack.pop(),label)]\n",
    "            else:\n",
    "                arcs[buffer[-1]].append((stack.pop(),label))\n",
    "        elif action=='right_arc':\n",
    "            if arcs.get(stack[-1])==None:\n",
    "                arcs[stack[-1]]=[(buffer[-1],label)]\n",
    "            else:\n",
    "                arcs[stack[-1]].append((buffer[-1],label))\n",
    "            stack.append(buffer.pop())\n",
    "    dependency_graph = defaultdict(lambda: defaultdict())\n",
    "    for word in reversed(sentence): #store in reverse in buffer, so top of buffer is first word\n",
    "        buffer.append(word)\n",
    "        dependency_graph[sentence[word]['head']][word]=sentence[word]['deprel']\n",
    "    head_found=dict()\n",
    "    while not len(buffer)==0:\n",
    "        if len(stack)>=1:\n",
    "            if stack[-1]!=0 and head_found.get(stack[-1])==None and dependency_graph.get(buffer[-1]) is not None and dependency_graph[buffer[-1]].get(stack[-1]) is not None:\n",
    "                head_found[stack[-1]]=True\n",
    "                perform_action(action='left_arc',label=dependency_graph[buffer[-1]][stack[-1]])\n",
    "            elif head_found.get(buffer[-1])==None and dependency_graph.get(stack[-1]) is not None and dependency_graph[stack[-1]].get(buffer[-1]) is not None:\n",
    "                head_found[buffer[-1]]=True\n",
    "                perform_action(action='right_arc',label=dependency_graph[stack[-1]][buffer[-1]])\n",
    "            elif head_found.get(stack[-1])!=None and not any([dependent not in head_found for dependent in dependency_graph[stack[-1]].keys()]):\n",
    "                perform_action(action='reduce')\n",
    "            else:\n",
    "                perform_action(action='shift')\n",
    "        else:\n",
    "            perform_action(action='shift')\n",
    "    return states,transitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature extraction baseline and additional<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_oneword(state,row,sentence):\n",
    "    buffer,stack,arcs=state\n",
    "    if len(stack)>1:\n",
    "        row['S0_w']=str(stack[-1])\n",
    "        row['S0_p']=sentence[stack[-1]]['xpos']\n",
    "        row['S0_w_p']=str(stack[-1])+'_'+sentence[stack[-1]]['xpos']\n",
    "    else:\n",
    "        row['S0_w']='None'\n",
    "        row['S0_p']='None'\n",
    "        row['S0_w_p']='None'\n",
    "    if len(buffer)>=1:\n",
    "        row['N0_w']=str(buffer[-1])\n",
    "        row['N0_p']=sentence[buffer[-1]]['xpos']\n",
    "        row['N0_w_p']=str(buffer[-1])+'_'+sentence[buffer[-1]]['xpos']\n",
    "    else:\n",
    "        row['N0_w']='None'\n",
    "        row['N0_p']='None'\n",
    "        row['N0_w_p']='None'\n",
    "    if len(buffer)>=2:\n",
    "        row['N1_w']=str(buffer[-2])\n",
    "        row['N1_p']=sentence[buffer[-2]]['xpos']\n",
    "        row['N1_w_p']=str(buffer[-2])+'_'+sentence[buffer[-2]]['xpos']\n",
    "    else:\n",
    "        row['N1_w']='None'\n",
    "        row['N1_p']='None'\n",
    "        row['N1_w_p']='None'\n",
    "    if len(buffer)>=3:\n",
    "        row['N2_w']=str(buffer[-3])\n",
    "        row['N2_p']=sentence[buffer[-3]]['xpos']\n",
    "        row['N2_w_p']=str(buffer[-3])+'_'+sentence[buffer[-3]]['xpos']\n",
    "    else:\n",
    "        row['N2_w']='None'\n",
    "        row['N2_p']='None'\n",
    "        row['N2_w_p']='None'\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_twoword(state,row,sentence):\n",
    "    buffer,stack,arcs=state\n",
    "    if len(stack)>1 and len(buffer)>=1:\n",
    "        row['S0_w_p_N0_w_p']=str(stack[-1])+'_'+sentence[stack[-1]]['xpos']+'_'+str(buffer[-1])+'_'+sentence[buffer[-1]]['xpos']\n",
    "        row['S0_w_p_N0_w']=str(stack[-1])+'_'+sentence[stack[-1]]['xpos']+'_'+str(buffer[-1])\n",
    "        row['S0_w_N0_w_p']=str(stack[-1])+'_'+str(buffer[-1])+'_'+sentence[buffer[-1]]['xpos']\n",
    "        row['S0_w_p_N0_p']=str(stack[-1])+'_'+sentence[stack[-1]]['xpos']+'_'+sentence[buffer[-1]]['xpos']\n",
    "        row['S0_p_N0_w_p']=sentence[stack[-1]]['xpos']+'_'+str(buffer[-1])+'_'+sentence[buffer[-1]]['xpos']\n",
    "        row['S0_w_N0_w']=str(stack[-1])+'_'+str(buffer[-1])\n",
    "        row['S0_p_N0_p']=sentence[stack[-1]]['xpos']+'_'+sentence[buffer[-1]]['xpos']\n",
    "    else:\n",
    "        row['S0_w_p_N0_w_p']='None'\n",
    "        row['S0_w_p_N0_w']='None'\n",
    "        row['S0_w_N0_w_p']='None'\n",
    "        row['S0_w_p_N0_p']='None'\n",
    "        row['S0_p_N0_w_p']='None'\n",
    "        row['S0_w_N0_w']='None'\n",
    "        row['S0_p_N0_p']='None'\n",
    "    if len(buffer)>=2:\n",
    "        row['N0_p_N1_p']=sentence[buffer[-1]]['xpos']+'_'+sentence[buffer[-2]]['xpos']\n",
    "    else:\n",
    "        row['N0_p_N1_p']='None'\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_threeword(state,row,sentence):\n",
    "    buffer,stack,arcs=state\n",
    "    if len(buffer)>=3:\n",
    "        row['N0_p_N1_p_N2_p']=sentence[buffer[-1]]['xpos']+'_'+sentence[buffer[-2]]['xpos']+'_'+sentence[buffer[-3]]['xpos']\n",
    "    else:\n",
    "        row['N0_p_N1_p_N2_p']='None'\n",
    "    if len(buffer)>=2 and len(stack)>1:\n",
    "        row['S0_p_N0_p_N1_p']=sentence[stack[-1]]['xpos']+'_'+sentence[buffer[-1]]['xpos']+'_'+sentence[buffer[-2]]['xpos']\n",
    "    else:\n",
    "        row['S0_p_N0_p_N1_p']='None'\n",
    "    s0h=-1\n",
    "    s0l=-1\n",
    "    s0r=-1\n",
    "    n0l=-1\n",
    "    if len(stack)>1:\n",
    "        for node in arcs:\n",
    "            for arc in arcs[node]:\n",
    "                if arc[0]==stack[-1]:\n",
    "                    s0h=node\n",
    "                    break\n",
    "            if s0h!=-1:\n",
    "                break\n",
    "        if arcs.get(stack[-1])!=None:\n",
    "            for arc in arcs[stack[-1]]:\n",
    "                if s0l==-1 and s0r==-1:\n",
    "                    s0l=arc[0]\n",
    "                    s0r=arc[0]\n",
    "                elif arc[0]<s0l:\n",
    "                    s0l=arc[0]\n",
    "                elif arc[0]>s0r:\n",
    "                    s0r=arc[0]\n",
    "    if len(buffer)>=1:\n",
    "        if arcs.get(buffer[-1])!=None:\n",
    "            for arc in arcs[buffer[-1]]:\n",
    "                if n0l==-1:\n",
    "                    n0l=arc[0]\n",
    "                elif arc[0]<n0l:\n",
    "                    n0l=arc[0]\n",
    "    if len(stack)>1 and len(buffer)>=1:\n",
    "        if s0h!=-1 and s0h!=0:\n",
    "            row['S0h_p_S0_p_N0_p']=sentence[s0h]['xpos']+'_'+sentence[stack[-1]]['xpos']+'_'+sentence[buffer[-1]]['xpos']\n",
    "        else:\n",
    "            row['S0h_p_S0_p_N0_p']='None'\n",
    "        if s0l!=-1 and s0l!=0:\n",
    "            row['S0_p_S0l_p_N0_p']=sentence[stack[-1]]['xpos']+'_'+sentence[s0l]['xpos']+'_'+sentence[buffer[-1]]['xpos']\n",
    "        else:\n",
    "            row['S0_p_S0l_p_N0_p']='None'\n",
    "        if s0r!=-1 and s0r!=0:\n",
    "            row['S0_p_S0r_p_N0_p']=sentence[stack[-1]]['xpos']+'_'+sentence[s0r]['xpos']+'_'+sentence[buffer[-1]]['xpos']\n",
    "        else:\n",
    "            row['S0_p_S0r_p_N0_p']='None'\n",
    "        if n0l!=-1:\n",
    "            row['S0_p_N0_p_N0l_p']=sentence[stack[-1]]['xpos']+'_'+sentence[buffer[-1]]['xpos']+'_'+sentence[n0l]['xpos']\n",
    "        else:\n",
    "            row['S0_p_N0_p_N0l_p']='None'\n",
    "    else:\n",
    "        row['S0h_p_S0_p_N0_p']='None'\n",
    "        row['S0_p_S0l_p_N0_p']='None'\n",
    "        row['S0_p_S0r_p_N0_p']='None'\n",
    "        row['S0_p_N0_p_N0l_p']='None'\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_distance(state,row,sentence):\n",
    "    buffer,stack,arcs=state\n",
    "    d='None'\n",
    "    if len(stack)>1 and len(buffer)>=1:\n",
    "        d=str(abs(stack[-1]-buffer[-1]))\n",
    "    if len(stack)>1:\n",
    "        row['S0_w_d']=str(stack[-1])+'_'+d\n",
    "        row['S0_p_d']=sentence[stack[-1]]['xpos']+'_'+d\n",
    "    else:\n",
    "        row['S0_w_d']='None'\n",
    "        row['S0_p_d']='None'\n",
    "    if len(buffer)>=1:\n",
    "        row['N0_w_d']=str(buffer[-1])+'_'+d\n",
    "        row['N0_p_d']=sentence[buffer[-1]]['xpos']+'_'+d\n",
    "    else:\n",
    "        row['N0_w_d']='None'\n",
    "        row['N0_p_d']='None'\n",
    "    if len(stack)>1 and len(buffer)>=1:\n",
    "        row['S0_w_N0_w_d']=str(stack[-1])+'_'+str(buffer[-1])+'_'+d\n",
    "        row['S0_p_N0_p_d']=sentence[stack[-1]]['xpos']+'_'+sentence[buffer[-1]]['xpos']+'_'+d\n",
    "    else:\n",
    "        row['S0_w_N0_w_d']='None'\n",
    "        row['S0_p_N0_p_d']='None'\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_valency(state,row,sentence):\n",
    "    buffer,stack,arcs=state\n",
    "    if len(stack)>1:\n",
    "        vl=0\n",
    "        vr=0\n",
    "        if arcs.get(stack[-1])!=None:\n",
    "            for arc in arcs[stack[-1]]:\n",
    "                if arc[0]<stack[-1]:\n",
    "                    vl+=1\n",
    "                elif arc[0]>stack[-1]:\n",
    "                    vr+=1\n",
    "        row['S0_w_vr']=str(stack[-1])+'_'+str(vr)\n",
    "        row['S0_p_vr']=sentence[stack[-1]]['xpos']+'_'+str(vr)\n",
    "        row['S0_w_vl']=str(stack[-1])+'_'+str(vl)\n",
    "        row['S0_p_vl']=sentence[stack[-1]]['xpos']+'_'+str(vl)\n",
    "    else:\n",
    "        row['S0_w_vr']='None'\n",
    "        row['S0_p_vr']='None'\n",
    "        row['S0_w_vl']='None'\n",
    "        row['S0_p_vl']='None'\n",
    "    if len(buffer)>=1:\n",
    "        vl=0\n",
    "        vr=0\n",
    "        if arcs.get(buffer[-1])!=None:\n",
    "            for arc in arcs[buffer[-1]]:\n",
    "                if arc[0]<buffer[-1]:\n",
    "                    vl+=1\n",
    "                elif arc[0]>buffer[-1]:\n",
    "                    vr+=1\n",
    "        row['N0_w_vr']=str(buffer[-1])+'_'+str(vr)\n",
    "        row['N0_p_vr']=sentence[buffer[-1]]['xpos']+'_'+str(vr)\n",
    "        row['N0_w_vl']=str(buffer[-1])+'_'+str(vl)\n",
    "        row['N0_p_vl']=sentence[buffer[-1]]['xpos']+'_'+str(vl)\n",
    "    else:\n",
    "        row['N0_w_vr']='None'\n",
    "        row['N0_p_vr']='None'\n",
    "        row['N0_w_vl']='None'\n",
    "        row['N0_p_vl']='None'\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_unigram(state,row,sentence):\n",
    "    buffer,stack,arcs=state\n",
    "    s0h=-1\n",
    "    s0l=-1\n",
    "    s0l_l=-1\n",
    "    s0r=-1\n",
    "    s0r_l=-1\n",
    "    n0l=-1\n",
    "    n0l_l=-1\n",
    "    s0_l=-1\n",
    "    if len(stack)>1:\n",
    "        for node in arcs:\n",
    "            for arc in arcs[node]:\n",
    "                if arc[0]==stack[-1]:\n",
    "                    s0h=node\n",
    "                    s0_l=arc[1]\n",
    "                    break\n",
    "            if s0h!=-1:\n",
    "                break\n",
    "        if arcs.get(stack[-1])!=None:\n",
    "            for arc in arcs[stack[-1]]:\n",
    "                if s0l==-1 and s0r==-1:\n",
    "                    s0l=arc[0]\n",
    "                    s0l_l=arc[1]\n",
    "                    s0r=arc[0]\n",
    "                    s0r_l=arc[1]\n",
    "                elif arc[0]<s0l:\n",
    "                    s0l=arc[0]\n",
    "                    s0l_l=arc[1]\n",
    "                elif arc[0]>s0r:\n",
    "                    s0r=arc[0]\n",
    "                    s0r_l=arc[1]\n",
    "    if len(buffer)>=1:\n",
    "        if arcs.get(buffer[-1])!=None:\n",
    "            for arc in arcs[buffer[-1]]:\n",
    "                if n0l==-1:\n",
    "                    n0l=arc[0]\n",
    "                    n0l_l=arc[1]\n",
    "                elif arc[0]<n0l:\n",
    "                    n0l=arc[0]\n",
    "                    n0l_l=arc[1]\n",
    "    if s0h!=-1 and s0h!=0:\n",
    "        row['S0h_w']=str(s0h)\n",
    "        row['S0h_p']=sentence[s0h]['xpos']\n",
    "    else:\n",
    "        row['S0h_w']='None'\n",
    "        row['S0h_p']='None'\n",
    "    if s0_l!=-1:\n",
    "        row['S0_l']=s0_l\n",
    "    else:\n",
    "        row['S0_l']='None'\n",
    "    if s0l!=-1 and s0l!=0:\n",
    "        row['S0l_w']=str(s0l)\n",
    "        row['S0l_p']=sentence[s0l]['xpos']\n",
    "        row['S0l_l']=s0l_l\n",
    "    else:\n",
    "        row['S0l_w']='None'\n",
    "        row['S0l_p']='None'\n",
    "        row['S0l_l']='None'\n",
    "    if s0r!=-1 and s0r!=0:\n",
    "        row['S0r_w']=str(s0r)\n",
    "        row['S0r_p']=sentence[s0r]['xpos']\n",
    "        row['S0r_l']=s0r_l\n",
    "    else:\n",
    "        row['S0r_w']='None'\n",
    "        row['S0r_p']='None'\n",
    "        row['S0r_l']='None'\n",
    "    if n0l!=-1 and n0l!=0:\n",
    "        row['N0l_w']=str(n0l)\n",
    "        row['N0l_p']=sentence[n0l]['xpos']\n",
    "        row['N0l_l']=n0l_l\n",
    "    else:\n",
    "        row['N0l_w']='None'\n",
    "        row['N0l_p']='None'\n",
    "        row['N0l_l']='None'\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_features(states,transitions,sentence,data,counter,label,fs):\n",
    "    for state,transition in zip(states,transitions):\n",
    "        row={}\n",
    "        if 'one' in fs:\n",
    "            row=extract_features_oneword(state,row,sentence)\n",
    "        if 'two' in fs:\n",
    "            row=extract_features_twoword(state,row,sentence)\n",
    "        if 'three' in fs:\n",
    "            row=extract_features_threeword(state,row,sentence)\n",
    "        if 'distance' in fs:\n",
    "            row=extract_features_distance(state,row,sentence)\n",
    "        if 'valency' in fs:\n",
    "            row=extract_features_valency(state,row,sentence)\n",
    "        if 'unigram' in fs:\n",
    "            row=extract_features_unigram(state,row,sentence)\n",
    "        if transition[1]=='':\n",
    "            label[counter]=transition[0]\n",
    "        else:\n",
    "            label[counter]=transition[0]+','+transition[1]\n",
    "        data[counter]=row\n",
    "        counter+=1\n",
    "    return counter\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Modified arc eager parsing for test data<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_eager_test_unshift(sentence,model,enc,fs):\n",
    "    buffer=list()\n",
    "    stack=[0]\n",
    "    arcs={}\n",
    "    def perform_action(action,label=''):\n",
    "        nonlocal buffer,stack,arcs\n",
    "        if action=='shift':\n",
    "            stack.append(buffer.pop())\n",
    "        elif action=='reduce':\n",
    "            stack.pop()\n",
    "        elif action=='left_arc':\n",
    "            if arcs.get(buffer[-1])==None:\n",
    "                arcs[buffer[-1]]=[(stack.pop(),label)]\n",
    "            else:\n",
    "                arcs[buffer[-1]].append((stack.pop(),label))\n",
    "        elif action=='right_arc':\n",
    "            if arcs.get(stack[-1])==None:\n",
    "                arcs[stack[-1]]=[(buffer[-1],label)]\n",
    "            else:\n",
    "                arcs[stack[-1]].append((buffer[-1],label))\n",
    "            stack.append(buffer.pop())\n",
    "        elif action=='unshift':\n",
    "            buffer.append(stack.pop())\n",
    "    for word in reversed(sentence): #store in reverse in buffer, so top of buffer is first word\n",
    "        buffer.append(word)\n",
    "    head_found=dict()\n",
    "    end_seen=False\n",
    "    while not (len(stack)==1 and len(buffer)==0 and end_seen):\n",
    "        if len(buffer)==0 and not end_seen:\n",
    "            end_seen=True\n",
    "            continue\n",
    "        state=(buffer,stack,arcs)\n",
    "        row={}\n",
    "        if 'one' in fs:\n",
    "            row=extract_features_oneword(state,row,sentence)\n",
    "        if 'two' in fs:\n",
    "            row=extract_features_twoword(state,row,sentence)\n",
    "        if 'three' in fs:\n",
    "            row=extract_features_threeword(state,row,sentence)\n",
    "        if 'distance' in fs:\n",
    "            extract_features_distance(state,row,sentence)\n",
    "        if 'valency' in fs:\n",
    "            row=extract_features_valency(state,row,sentence)\n",
    "        if 'unigram' in fs:\n",
    "            row=extract_features_unigram(state,row,sentence)\n",
    "        data={0:row}\n",
    "        df = pd.DataFrame.from_dict(data,'index')\n",
    "        x=enc.transform(df)\n",
    "        decision_probs=np.argsort(-1 *model.predict_proba(x)[0],kind='quicksort')\n",
    "        for decision in decision_probs:\n",
    "            temp=model.classes_[decision].split(',')\n",
    "            transition=temp[0]\n",
    "            label=temp[1] if len(temp)>1 else ''\n",
    "            if end_seen and head_found.get(stack[-1])==None and  len(buffer)==0:\n",
    "                perform_action(action='unshift')\n",
    "                break\n",
    "            if transition=='left_arc':\n",
    "                if stack[-1]!=0 and len(buffer)>=1 and head_found.get(stack[-1])==None and len(stack)>=1:\n",
    "                    head_found[stack[-1]]=(buffer[-1],label)\n",
    "                    perform_action(action='left_arc',label=label)\n",
    "                    break\n",
    "            if transition=='right_arc':\n",
    "                if len(stack)>=1 and len(buffer)>=1 and head_found.get(buffer[-1])==None:\n",
    "                    head_found[buffer[-1]]=(stack[-1],label)\n",
    "                    perform_action(action='right_arc',label=label) \n",
    "                    break\n",
    "            if transition=='reduce':\n",
    "                if head_found.get(stack[-1])!=None and len(stack)>1:\n",
    "                    perform_action(action='reduce')\n",
    "                    break\n",
    "            if transition=='shift' and not end_seen:\n",
    "                perform_action(action='shift')\n",
    "                break\n",
    "    return head_found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Arc eager parsing for test data<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_eager_test(sentence,model,enc,fs):\n",
    "    buffer=list()\n",
    "    stack=[0]\n",
    "    arcs={}\n",
    "    def perform_action(action,label=''):\n",
    "        nonlocal buffer,stack,arcs\n",
    "        if action=='shift':\n",
    "            stack.append(buffer.pop())\n",
    "        elif action=='reduce':\n",
    "            stack.pop()\n",
    "        elif action=='left_arc':\n",
    "            if arcs.get(buffer[-1])==None:\n",
    "                arcs[buffer[-1]]=[(stack.pop(),label)]\n",
    "            else:\n",
    "                arcs[buffer[-1]].append((stack.pop(),label))\n",
    "        elif action=='right_arc':\n",
    "            if arcs.get(stack[-1])==None:\n",
    "                arcs[stack[-1]]=[(buffer[-1],label)]\n",
    "            else:\n",
    "                arcs[stack[-1]].append((buffer[-1],label))\n",
    "            stack.append(buffer.pop())\n",
    "    for word in reversed(sentence): #store in reverse in buffer, so top of buffer is first word\n",
    "        buffer.append(word)\n",
    "    head_found=dict()\n",
    "    end_seen=False\n",
    "    while len(buffer)!=0:\n",
    "        state=(buffer,stack,arcs)\n",
    "        row={}\n",
    "        if 'one' in fs:\n",
    "            row=extract_features_oneword(state,row,sentence)\n",
    "        if 'two' in fs:\n",
    "            row=extract_features_twoword(state,row,sentence)\n",
    "        if 'three' in fs:\n",
    "            row=extract_features_threeword(state,row,sentence)\n",
    "        if 'distance' in fs:\n",
    "            extract_features_distance(state,row,sentence)\n",
    "        if 'valency' in fs:\n",
    "            row=extract_features_valency(state,row,sentence)\n",
    "        if 'unigram' in fs:\n",
    "            row=extract_features_unigram(state,row,sentence)\n",
    "        data={0:row}\n",
    "        df = pd.DataFrame.from_dict(data,'index')\n",
    "        x=enc.transform(df)\n",
    "        decision_probs=np.argsort(-1 *model.predict_proba(x)[0],kind = 'quicksort')\n",
    "        \n",
    "        left_arc_possible=stack[-1]!=0 and head_found.get(stack[-1])==None and len(stack)>=1\n",
    "        right_arc_possible=len(stack)>=1 and head_found.get(buffer[-1])==None\n",
    "        reduce_possible=head_found.get(stack[-1])!=None and len(stack)>=1\n",
    "        for decision in decision_probs:\n",
    "            temp=model.classes_[decision].split(',')\n",
    "            transition=temp[0]\n",
    "            label=temp[1] if len(temp)>1 else ''\n",
    "            if transition=='left_arc':\n",
    "                if left_arc_possible:\n",
    "                    head_found[stack[-1]]=(buffer[-1],label)\n",
    "                    perform_action(action='left_arc',label=label)\n",
    "                    break\n",
    "            if transition=='right_arc':\n",
    "                if right_arc_possible:\n",
    "                    head_found[buffer[-1]]=(stack[-1],label)\n",
    "                    perform_action(action='right_arc',label=label) \n",
    "                    break\n",
    "            if transition=='reduce':\n",
    "                if reduce_possible:\n",
    "                    perform_action(action='reduce')\n",
    "                    break\n",
    "            if transition=='shift':\n",
    "                perform_action(action='shift')\n",
    "                break\n",
    "    return head_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(fs):\n",
    "    sentences=get_sentences(train_file_stat)\n",
    "    filt_sentences=filter_non_projective(sentences)\n",
    "    data={}\n",
    "    label={}\n",
    "    counter=0\n",
    "    for i in tqdm(range(len(filt_sentences))):\n",
    "        states,transitions=parse_eager(filt_sentences[i])\n",
    "        counter=extract_features(states,transitions,filt_sentences[i],data,counter,label,fs)\n",
    "    df = pd.DataFrame.from_dict(data,'index')\n",
    "    label=np.array(list(label.values()))\n",
    "    enc = OneHotEncoder(handle_unknown='ignore',sparse=True,dtype=int)\n",
    "    x=enc.fit_transform(df)\n",
    "    #model= BernoulliNB()\n",
    "    model=LogisticRegression(verbose = 1,n_jobs = 6)\n",
    "    model.fit(x,label)\n",
    "    return model,enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(fs,unshift=0):\n",
    "    model,enc=get_model(fs)\n",
    "    sentences=get_sentences(test_file_stat)\n",
    "    filt_sentences=filter_non_projective(sentences)\n",
    "    total=0\n",
    "    labelled=0\n",
    "    unlabelled=0\n",
    "    total_nodes=0\n",
    "    for i in tqdm(range(len(filt_sentences))):\n",
    "        sentence=filt_sentences[i]\n",
    "        total_nodes+=len(sentence)\n",
    "        heads=''\n",
    "        if unshift==1:\n",
    "            heads=parse_eager_test_unshift(sentence,model,enc,fs)\n",
    "        else:\n",
    "            heads=parse_eager_test(sentence,model,enc,fs)\n",
    "        for node in heads:\n",
    "            head,label=heads[node]\n",
    "            label=label.split(':')[0]\n",
    "            total+=1\n",
    "            if sentence[node]['head']==head:\n",
    "                unlabelled+=1\n",
    "                if sentence[node]['deprel'].split(':')[0]==label:\n",
    "                    labelled+=1\n",
    "    p=labelled/total\n",
    "    r=labelled/total_nodes\n",
    "    las_score=2*p*r/(p+r)\n",
    "    p=unlabelled/total\n",
    "    r=unlabelled/total_nodes\n",
    "    uas_score=2*p*r/(p+r)\n",
    "    info={}\n",
    "    info['gs_nodes']=total_nodes\n",
    "    info['sp_nodes']=total\n",
    "    info['correct_labelled']=labelled\n",
    "    info['correct_unlabelled']=unlabelled\n",
    "    info['UAS_score']=uas_score\n",
    "    info['LAS_score']=las_score\n",
    "    info['features']=fs\n",
    "    info['unshift']=unshift\n",
    "    try:\n",
    "        f = open('Scores.txt', 'a')\n",
    "        f.write(str(info))\n",
    "        f.close()\n",
    "    except:\n",
    "        print(\"Unable to append to file\")\n",
    "    return info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Code block for testing<h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add features to the feature array to be used for training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "features that can be added are:'one','two','three','valency','distance','unigram'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "change the unshift option to 1 to use the modified Arc eager approach for parsing or 0 to use the normal Arc eager parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All results are saved to a file called Scores.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0d67fcfea49417e9d9786b65b9ff0d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11467.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   1 out of   1 | elapsed:  5.4min finished\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7e750f532da4ba9b1c2cb37356e3b06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1473.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_array=['one','two','three','valency','distance','unigram']\n",
    "results=get_scores(feature_array,unshift=1)\n",
    "print('gs_nodes',results['gs_nodes'])\n",
    "print('sp_nodes',results['sp_nodes'])\n",
    "print('correct_labelled',results['correct_labelled'])\n",
    "print('correct_unlabelled',results['correct_unlabelled'])\n",
    "print('UAS_score',results['UAS_score'])\n",
    "print('LAS_score',results['LAS_score'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
